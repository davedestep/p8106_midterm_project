---
title: "p8106_midterm_project"
author: "David DeStephano"
date: "March 27, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction
Hosptial billing is often nebulous, and differs dramatically from actual costs. The purpose of this project is to use SPARCS deidentified data to predict total charges of service for newborns in NYS. We hope to achieve a model that performs relatively well on the majority of observations, and performs reasonably well on high outliers, of which there are many. We also hope to determine if demographic and insurance characteristics are influential on charges (not costs of service), to see if patient and payer characteristics affect hospital inpatient charges.

Data was downloaded from the NYS health data website: https://health.data.ny.gov/Health/Hospital-Inpatient-Discharges-SPARCS-De-Identified/npsr-cm47

The data was cleaned by reoving any observation without a missing birthweight variable, to limit the analysis to newborns only. The data had variables for diagnosis and procedure codes as well as descriptions, so redundant variables were removed. Descriptions were kept as to aid with interpretation.

Variables with too many categories such as zip code and hospital name were removed. The remainder of categorical variables were then dummified to ensure the variables would work well with Caret. To avoid multicolinearity the option fullRank=T was used in the dummyVar function.

Due to the high amount of categories in some of these variables, categories with zero or near zero variances were examined. They were then determined to not improve model performance significantly, and were removed in subsequent models. The variables removed were CCS diagnosis and procedure codes, as well as the APR DRG variable. This left the model with multilevel (CCS) variables, while single level variables were removed.  


#Exploratory Analysis
As expected, charges vastly surpass the actual costs of inpatient stays for newborns. There appear to be slight differences in slopes between cost and charges for different races, but this can not be tested to be meaningful at this stage. There also appear to be differences between payment types, meaning that demographic and payment typology may interact with costs/charges may not be uniform between groups.

#Models
Many variables were dropped for the final model, none of which significantly increased model performance. The final model containted the following predictors:

 [1] "apr_mdc_description"                
 [2] "apr_medical_surgical_description"   
 [3] "apr_risk_of_mortality"              
 [4] "apr_severity_of_illness_description"
 [5] "birth_weight"                       
 [6] "emergency_department_indicator"     
 [7] "ethnicity"                          
 [8] "gender"                             
 [9] "health_service_area"                
[10] "hospital_county"                    
[11] "length_of_stay"                     
[12] "patient_disposition"                
[13] "payment_typology_1"                 
[14] "race"                               
[15] "total_costs"                        
[16] "type_of_admission"

A linear model and PCR were fit initially to test the performance when including APR DRG, and CCS variables. These variables were removed, and a linear model, PCR, lasso, and elastic net were used on the dummified dataset. Tuning parameters were choses using 5 fold cross validation A GAM model was fit as well to account for possible non linearity.

Test performance RMSE did not change between the regularized models, while MAE was slightly lower in the elastic net and lasso models. RMSE was much lower in the GAM model compared to the regularized and linear models. Comparisons can be found below in the attached R markdown code and output.

Obviously cost was important in predicting total charges, but the other continuous variables were also important, including length of stay and birthweight. Risk of mortality and severity were also important predictors.

Limitations include extreme outlying charges. Tese observations resuted in high errors, as they were much more difficult to accurately predict. The models also did not include interaction terms, so there was no test of whether there is an interaction between cost and race or payment typology.

#Conclusion
The GAM model fit the data the best and had the lowest RMSE and MAE. The other regularized and linear models performed about the same. I expected the regularized models to fit the best, because there are a large amount of dummy variables that I expected would have their coefficients shrunk.


## Load the data set
Tips for dealing with large data https://rpubs.com/msundar/large_data_analysis 

```{r message=FALSE}
library(tidyverse)
library(caret)
library(summarytools)
library(cowplot)
library(ModelMetrics)
library(pls)

sparcs<-read_csv("Hospital_Inpatient_Discharges__SPARCS_De-Identified___2013.csv")

sparcs %>% janitor::clean_names() %>%  ggplot(aes(x = age_group, y=total_charges, fill = age_group))+
  geom_boxplot() +ylim(0,50000)

```



##2,000,000 rows is excessive, and I do not want to wait forever when running models. So I am just going to look at newborns and see if we can predict costs
```{r}
newborns<-sparcs %>% filter(`Birth Weight`!="0000") %>% janitor::clean_names() 
rm(sparcs)
```

##Descriptive Stats and Visualization
```{r warning=FALSE}

a<-newborns %>% ggplot(aes(x=total_costs))+
  geom_histogram()+xlim(0,50000)

b<-newborns %>% ggplot(aes(x=total_charges))+
  geom_histogram()+xlim(0,50000)

require(gridExtra)
grid.arrange(a,b)
# ggplot(newborns, aes(x = total_charges, y = ..density..)) + 
#     stat_density(geom="line")  +xlim(0,50000)



newborns %>% ggplot(aes(x = race, y=total_charges, fill = ethnicity))+
  geom_boxplot() +ylim(0,50000)


newborns %>%  
  ggplot(aes(x = total_costs, y=total_charges, group=race, color=race))+
  geom_smooth(se=FALSE)+ylim(0,20000) + xlim(0, 10000)



newborns  %>%
  ggplot(aes(x = total_costs, y=total_charges, group=payment_typology_1, color=payment_typology_1))+
  geom_smooth(se=FALSE)+ylim(0,200000) + xlim(0, 100000)

```


#Remove redundant categorical variables, dummify the remaining categorical variables, and remove zero variance predictors
```{r}
newborn_model<-newborns %>% select(-operating_certificate_number, -facility_id, -discharge_year, -age_group, -ccs_diagnosis_code, -ccs_procedure_code, -apr_drg_code, -apr_mdc_code, -apr_severity_of_illness_code, -payment_typology_2, -payment_typology_3, -abortion_edit_indicator) %>% 
  mutate(birth_weight=as.numeric(birth_weight),
         length_of_stay=as.numeric(length_of_stay)) %>% 
  na.omit() #Don't feel like imputing

#A lot of categorical variables:
#facility_name
#zip code
#remove apr_drg_description?

newborn_model <- newborn_model %>% select(-facility_name, -zip_code_3_digits)

dmy<- dummyVars(" ~.", data=newborn_model, fullRank = T)
trsf <- data.frame(predict(dmy, newdata = newborn_model))

#zero_var = nearZeroVar(trsf, saveMetrics = TRUE)
#23821.100



```


There are many near zero variance dummy variables. There were several ideas on how to deal with this: could remove any dummy variable with less than ten observations using nearZeroVar(), as they have very low variance, this will help speed up the model.. This could severley affect the reference as there are many cost outliers.. So under normal circumstances this would not be ideal... Should maybe just delete the patients with these variables? Or could just rerun the analysis later and exclude ccs_procedure ccs_diagnosis apr_drg variables (the variables with the most dummies) and compare the RMSE. Will try this after running a linear model and PCR, with 3 fold CV, as to save computing power on the 500 variable dataset. 



#Create test and training data
```{r}
set.seed(1)
rowTrain <-createDataPartition(y = trsf$total_charges,
                               p = 2/3,
                               list = FALSE)

# training data
x <- model.matrix(total_charges~.,trsf)[rowTrain,-1]
y <- trsf$total_charges[rowTrain]

# test data
x2 <- model.matrix(total_charges~.,trsf)[-rowTrain,-1]
y2 <- trsf$total_charges[-rowTrain]

```




#Linear model using CCS and APR DRG categories
##Commented out due to computation time
```{r}
# ctrl1 <-trainControl(method = "cv", number = 3)
# set.seed(1)
# 
# lm.fit <-train(x, y, method = "lm",trControl = ctrl1)
# 
# predy2.lm <-predict(lm.fit, newdata = x2)
# 
# mae(y2, predy2.lm)
# mse(y2, predy2.lm)
# rmse(y2, predy2.lm)
```

10 fold CV:
MAE: [1] 7492.759
MSE: [1] 758991280
RMSE: [1] 27549.8

By comparison, here are the estimates if you were to model cost instead of charges:
MAE: [1] 1863.642
MSE  [1] 43849131
RMSE [1] 6621.868
Obviously charges can vary more wildly



#PCR using CCS and APR DRG variables
```{r}
# set.seed(1)
# pcr.fit <- train(x, y,
#                  method = "pcr",
#                  tuneGrid = data.frame(ncomp = 1:425),
#                  trControl = ctrl1,
#                  preProc = c("center", "scale"))
# 
# predy2.pcr4 <- predict(pcr.fit, newdata = x2)
# 
# mae(y2, predy2.pcr4)
# mse(y2, predy2.pcr4)
# rmse(y2, predy2.pcr4)
# 
# ggplot(pcr.fit, highlight = TRUE) + theme_bw()
# pcr.fit$bestTune
# 
# pcr.fit$results %>% filter(ncomp==397)

```

On seperate test data:
MAE:  [1] 8055.074
MSE:  [1] 840093694
RMSE: [1] 28984.37


397 was the ideal numberof components, this is very large..
Error using pcr.fit$results %>% filter(ncomp==413)
MAE:  7454.877
RMSE: 27168
Rsquared: 0.835


When getting RMSE from separate test data, RMSE is 28984, linear model was 27549.8, but this model has a lower MAE. Regardless, PCR does not seem to improve the accuracy significantly. Will compare these numbers to model without CCS and APR DRG variables.




#Try the analysis again without the ccs_procedure, ccs_diagnosis, and apr_drg variables
##Choice to try this method and compare instead of either dropping dummy variables with low freqs or dropping patients with those categories
```{r}
newborn_model2 <- newborn_model %>% select(-ccs_diagnosis_description, -ccs_procedure_description, -apr_drg_description)

dmy2<- dummyVars(" ~.", data=newborn_model2, fullRank = T)
trsf2 <- data.frame(predict(dmy2, newdata = newborn_model2))

```

```{r}
set.seed(1)
rowTrain <-createDataPartition(y = trsf2$total_charges,
                               p = 2/3,
                               list = FALSE)

# training data
x <- model.matrix(total_charges~.,trsf2)[rowTrain,-1]
y <- trsf$total_charges[rowTrain]

# test data
x2 <- model.matrix(total_charges~.,trsf2)[-rowTrain,-1]
y2 <- trsf$total_charges[-rowTrain]

```

#Linear model
```{r error=FALSE}
ctrl1 <-trainControl(method = "cv", number = 5)
#ctrl1 <-trainControl(method = "none")
set.seed(1)

lm.fit2 <-train(x, y, method = "lm",trControl = ctrl1)

predy2.lm2 <-predict(lm.fit2, newdata = x2)

mae(y2, predy2.lm2)
mse(y2, predy2.lm2)
rmse(y2, predy2.lm2)
```

Very similar RMSE to before.


#PCR
```{r}
set.seed(1)
pcr.fit2 <- train(x, y,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:118), 
                 trControl = ctrl1,
                 preProc = c("center", "scale"))

pred.pcr2 <- predict(pcr.fit2, newdata = x2)

mae(y2, pred.pcr2)
mse(y2, pred.pcr2)
rmse(y2, pred.pcr2)

ggplot(pcr.fit2, highlight = TRUE) + theme_bw()
pcr.fit2$bestTune

pcr.fit2$results %>% filter(ncomp==114)

```

Removing the CCS and APR variables provides a lower RMSE and MAE compared to the previous model with these variables included. I think computationally this model is a better choice, and we have evidence that it actually performs better. We only lose 0.2% from our Rsquared value. 

This tells us that perhaps the super-specific diagnosis and procedure codes are not highly predicitve of charges, this somewhat defies expectations, as billing is generally completed using billing codes. This could just be the case for infants, as length of stay and severity are probably much greater factors of charges, while procedures are disproportionately and more or less uniformly deliveries, with some complications explainig the higher charges.

```{r}
importance = varImp(pcr.fit2, scale=FALSE)
importance
```


We only needed 114 components this time, which is still not great so will try alternative models.


#Lasso
```{r error=FALSE}
set.seed(1)
lasso.fit <- train(x, y,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-1, 5, length=100))),
                   preProc = c("center", "scale"),
trControl = ctrl1)


ggplot(lasso.fit, highlight = TRUE) + theme_bw()
lasso.fit$bestTune
coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```


I could not tell you why the coefficient estimate is 30000 for cost, is this for every dollar of cost? Or did something happen with the standardization process?

#Enet
```{r error=FALSE}
set.seed(1)
enet.fit <- train(x, y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 5),
                                         lambda = exp(seq(-2, 6, length = 50))),
                  preProc = c("center", "scale"),
                  trControl = ctrl1)

pred.enet <- predict(enet.fit, newdata = x2)

mae(y2, pred.enet)
mse(y2, pred.enet)
rmse(y2, pred.enet)

#enet.fit$results
```


```{r}
enet.fit$bestTune
ggplot(enet.fit, highlight = TRUE) + theme_bw()
coef(enet.fit$finalModel,enet.fit$bestTune$lambda)
```



There is barely a difference between any of the regularized models.


#Non linear model
```{r}
library(mgcv)

gam.m <-gam(total_charges~s(total_costs)+s(length_of_stay)+s(birth_weight), data = trsf2)
plot(gam.m)


gam.fit <-train(x, y,
                method = "gam",
                tuneGrid =data.frame(method = "GCV.Cp", select =c(TRUE,FALSE)),
                trControl = ctrl1)

```

```{r}
resamp <-resamples(list(lm = lm.fit2, PCR = pcr.fit2, lasso=lasso.fit, enet=enet.fit, gam=gam.fit))
summary(resamp)

bwplot((resamp), metric = "RMSE")

bwplot((resamp), metric = "MAE")

```
